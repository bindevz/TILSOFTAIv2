schema_version: "1.0"
document: "PATCH 08 - Implement OpenAI-Compatible Endpoint /v1/chat/completions (non-stream + stream)"
language: "en"

goal:
  - "Implement OpenAI-compatible Chat Completions adapter to integrate external clients/SDKs."
  - "Endpoint MUST call ChatPipeline (platform orchestration) and preserve multi-tenant/multi-language context."
  - "Support both non-stream and stream (SSE) compatible with OpenAI style."

target_repository_root: "/mnt/data/tilsoft_ai_review"
api_project: "src/TILSOFTAI.Api"
orchestration_entrypoint: "TILSOFTAI.Orchestration.Pipeline.ChatPipeline"

hard_constraints:
  - "No domain logic in controller."
  - "No prompt hacking in controller."
  - "Controller MUST rely on ExecutionContextMiddleware and ExceptionHandlingMiddleware."
  - "If request includes conversation id header, use that; else middleware generates."

endpoint_contract:
  route: "POST /v1/chat/completions"
  auth: "JWT Bearer required ([Authorize])"
  content_type: "application/json"
  request_shape (minimum supported):
    fields:
      model: "string (ignored or validated; platform uses appsettings Llm.Model)"
      messages: "array of { role: 'system'|'user'|'assistant', content: string }"
      stream: "boolean default false"
      temperature: "number optional (ignored; platform uses appsettings)"
      max_tokens: "int optional (ignored; platform uses appsettings)"
    rules:
      - "If multiple user messages exist, concatenate them in chronological order separated by '\n'."
      - "Ignore any request-provided tool definitions; platform uses ToolRegistry/ToolCatalog resolved tools."
      - "Ignore any request-provided system prompt; platform uses platform system prompt and policies."
  response_shape non-stream (OpenAI style):
    fields_required:
      - "id"
      - "object"
      - "created"
      - "model"
      - "choices[0].message.role='assistant'"
      - "choices[0].message.content"
      - "choices[0].finish_reason"
  response_shape stream (OpenAI SSE style):
    sse_rules:
      - "Set Content-Type: text/event-stream"
      - "Send events lines: 'data: <json>\n\n'"
      - "End with 'data: [DONE]\n\n'"
    chunk_shape (minimum):
      - "object: 'chat.completion.chunk'"
      - "choices[0].delta.content (string fragments)"
      - "choices[0].finish_reason (null until done)"

mapping_rules:
  from_openai_messages_to_platform_chat_request:
    - "Take the last user message as primary input."
    - "If earlier user messages exist, prepend them as context to input (joined by newline)."
    - "If the last message is not 'user', throw ArgumentException."
  from_platform_stream_events_to_openai_chunks:
    - "For each ChatStreamEvent.Delta(payload string): emit one OpenAI chunk with delta.content=payload."
    - "Ignore tool_call/tool_result events in OpenAI stream output (do not leak internal tooling by default)."
    - "When ChatStreamEvent.Final occurs: emit final chunk with finish_reason='stop' and then [DONE]."
    - "When ChatStreamEvent.Error occurs: terminate with [DONE] and rely on middleware for error response if not already started."

files_to_create_or_modify:
  create:
    - path: "src/TILSOFTAI.Api/Contracts/OpenAi/OpenAiChatCompletionsRequest.cs"
      content_requirements:
        - "DTO for request: Model, Messages, Stream."
        - "Messages element DTO: Role, Content."
        - "Use System.Text.Json attributes for camelCase."
    - path: "src/TILSOFTAI.Api/Contracts/OpenAi/OpenAiChatCompletionsResponse.cs"
      content_requirements:
        - "DTO for non-stream response with minimal OpenAI fields."
        - "Include id, object, created (unix seconds), model, choices."
    - path: "src/TILSOFTAI.Api/Contracts/OpenAi/OpenAiChatCompletionsStreamChunk.cs"
      content_requirements:
        - "DTO for stream chunk: id, object, created, model, choices[delta]."
    - path: "src/TILSOFTAI.Api/Streaming/OpenAiSseWriter.cs"
      content_requirements:
        - "Helper to write OpenAI SSE chunks + [DONE]."
        - "Must flush after each write."
  modify:
    - path: "src/TILSOFTAI.Api/Controllers/OpenAiChatCompletionsController.cs"
      changes:
        - "Remove NotImplementedException; implement the endpoint."
        - "Inject: ChatPipeline, IExecutionContextAccessor, ILogger<OpenAiChatCompletionsController>."
        - "For non-stream: call pipeline (Stream=false) and map to OpenAI response."
        - "For stream: call pipeline (Stream=true) and map delta/final to OpenAI SSE chunk events."
        - "Do not output tool calls/results in OpenAI stream."
    - path: "src/TILSOFTAI.Api/Extensions/MapTilsoftAiExtensions.cs"
      changes:
        - "Ensure /v1/chat/completions is mapped (controllers already)."

implementation_steps:
  - step: "S1 Create OpenAI DTOs and OpenAiSseWriter"
    detail:
      - "Use System.Text.Json for serialization."
      - "Set unix created time using DateTimeOffset.UtcNow.ToUnixTimeSeconds()."
  - step: "S2 Implement message mapping"
    detail:
      - "Validate request.messages exists and contains at least one user message."
      - "Take all user messages content and join with newline; last user message must be last in array."
      - "If last message role != 'user', throw ArgumentException."
  - step: "S3 Implement non-stream response"
    detail:
      - "ChatRequest.Input = joined user content; Stream=false."
      - "Call ChatPipeline.RunAsync(...)."
      - "Return OpenAiChatCompletionsResponse with assistant content."
  - step: "S4 Implement stream response"
    detail:
      - "Set Response.ContentType text/event-stream."
      - "Create Progress<ChatStreamEvent> and translate delta->chunk; final->chunk + done."
      - "Ignore tool events; optionally log them server-side."
      - "Write [DONE] at completion."

acceptance_criteria:
  - "POST /v1/chat/completions with non-stream returns OpenAI-shaped JSON."
  - "POST /v1/chat/completions with stream=true returns SSE chunks and ends with [DONE]."
  - "Responses honor ctx.Language indirectly via platform (assistant replies in ctx.Language)."
  - "No internal tool payloads are leaked to OpenAI stream by default."

tests_to_add:
  - "Integration test: non-stream OpenAI endpoint returns JSON with choices[0].message.content non-empty using mocked ILlmClient."
  - "Integration test: stream OpenAI endpoint returns 'data:' chunks and [DONE]."
